---
title: 'STAT 636: Final Project'
author: "Katie Hill, Asmita Nagila, Jerrin Wiley"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(ggplot2) #Plots for visualization
library(glmnet)
library(corrplot) #Correlation Heatmap
library(pROC) #Roc Curve
library(e1071)
library(caret)
```

## Project Data Source

<https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success>

Instances: 4424

Features: 36

Goal: Classify Students into 3 categories: Dropout, Enrolled, and Graduate

## Data Import

Import Data

```{r}
data <- read.csv("data.csv",sep=";")
attach(data)
```

```{r}
head(data)
```

Check for Missing Values

```{r}
which(is.na(data))
```

No line is missing data

Convert Target values to numerical values. Dropout = 1, Enrolled = 2, Graduate = 3

```{r}
model_data <- data
model_data$Target <- as.numeric(factor(Target))
```

## Data Exploration

```{r}
ggplot(data=data)+geom_bar(mapping=aes(x=Target))
```

Build function to graph distribution of feature by target

```{r}
FeatureVsTarget = function(feature,name){
  print(ggplot(data=data)+
          geom_boxplot(mapping=aes(x=reorder(Target,feature,FUN=median),y=feature))+xlab("Target")+ylab(name))
}
```

Plot Features

```{r}
data_col <- colnames(model_data)
for(i in 1:ncol(model_data)){
  FeatureVsTarget(model_data[,i], data_col[i])
}
```

```{r}
numericVars <- which(sapply(model_data, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on
cat('There are', length(numericVars), 'numeric variables')
```

```{r}

all_numVar <- model_data[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs")

cor_sorted <- as.matrix(sort(cor_numVar[,'Target'], decreasing = TRUE))

 #select highest correlations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.2)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")
```

## Data Split

70% train, 30% split (Will not be used for feature selection, k-fold cross-validation will be)

```{r}
sample <- sample(c(TRUE,FALSE),nrow(data),replace=TRUE,prob=c(0.7,0.3))
train <- data[sample,]
test <- data[!sample,]
```

## Data Prep

Considering dropping highly correlated variable to counteract their possible dependent relationship. First, find correlations over 0.85 within X

```{r fig.height=5, fig.width=5}

x <- all_numVar
data_cor <- cor(x)

#Create function to find correlations over 0.85
corr_find <- function(x){
  return (ifelse(x>=0.85,ifelse(x==1,0,1),0))
}

#high_corr <- data_cor[colSums(abs(data_cor))>0.85,]
data_cor[data_cor==1]<-0
data_cor[data_cor>=0.85]<-1
data_cor[data_cor<0.85]<-0
high_corr<-as.data.frame(data_cor)
high_corr <- high_corr[colSums(abs(high_corr),na.rm=TRUE)>0]
high_corr <- high_corr[rowSums(abs(high_corr))>0,]
corrplot(as.matrix(high_corr))
```

Next, for all pairs find the feature with the lowest correlation to Target

-   mother.s.occupation vs father.s.occupation = 0.0056 vs 0.0019

    -   father.s.occupation

-   Curricular.units.1st.sem..credited. vs Curricular.units.2nd.sem..credited. = 0.0481 vs 0.0540

    -   Curricular.units.1st.sem..credited.

-   Curricular.units.1st.sem..enrolled. vs Curricular.units.2nd.sem..enrolled. =0.1560 vs 0.1758

    -   Curricular.units.1st.sem..enrolled

-   Curricular.units.1st.sem..approved. vs Curricular.units.2nd.sem..approved. = 0.5291 vs 0.6242

    -   Curricular.units.1st.sem..approved.

```{r}
abs(cor_sorted["Mother.s.occupation",])
abs(cor_sorted["Father.s.occupation",])

abs(cor_sorted["Curricular.units.1st.sem..credited.",])
abs(cor_sorted["Curricular.units.2nd.sem..credited.",])

abs(cor_sorted["Curricular.units.1st.sem..enrolled.",])
abs(cor_sorted["Curricular.units.2nd.sem..enrolled.",])

abs(cor_sorted["Curricular.units.1st.sem..approved.",])
abs(cor_sorted["Curricular.units.2nd.sem..approved.",])
```

Drop lowest correlated of each pair

```{r}
dropFeatures <- c("Mother.s.occupation","Father.s.occupation","Curricular.units.1st.sem..credited.","Curricular.units.2nd.sem..credited.","Curricular.units.1st.sem..enrolled.","Curricular.units.2nd.sem..enrolled.","Curricular.units.1st.sem..approved.","Curricular.units.2nd.sem..approved.")
model_data <- model_data[,!names(model_data) %in% dropFeatures]
```

Create test and train set from cleaned data

```{r}
sample2 <- sample(c(TRUE,FALSE),nrow(model_data),replace=TRUE,prob=c(0.7,0.3))
train_clean <- data[sample2,]
test_clean <- data[!sample2,]
```

## Feature Selection - LASSO model

For the Lasso model, we will use the `glmnet` package. The model will be trained on the training set and evaluated on the test set.

```{r}
# Lasso Model Fitting

# Preparing the data for the Lasso model: matrix format is required for glmn

X_train <- model.matrix(~ . - 1, data = train[, -which(colnames(train) == "Target")]) 
Y_train <- train$Target

# Fit the Lasso model using cross-validation 
set.seed(1217) #for reproducibility
cv_model <- cv.glmnet(X_train, Y_train, family = "multinomial", type.multinomial = "grouped")

# Determine the best lambda (regularization parameter)
best_lambda <- cv_model$lambda.min

# Fit the final model on the training data using the best lambda
final_model <- glmnet(X_train, Y_train, family = "multinomial", lambda = best_lambda, type.multinomial = "grouped")

# View the model coefficients
coef(final_model, s = best_lambda)

plot(cv_model)
```

Testing the LASSO model

```{r}
# Preparing the test data in matrix format
X_test <- model.matrix(~ . - 1, data = test[, -which(colnames(test) == "Target")]) 
Y_test <- test$Target

# Making predictions on the test data
predictions <- predict(final_model, newx = X_test, s = best_lambda, type = "response")

# Converting predictions to the same format as 'Y_test' for comparison
predicted_classes <- apply(predictions, 1, which.max)

# Evaluate the model performance
confusion_matrix <- table(Predicted = predicted_classes, Actual = Y_test)
print(confusion_matrix)

# Calculating overall accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy: ", accuracy)

```

## Support Vector Machine (SVM) model

```{r}
# Convert target variable to a facor
train$Target <- as.factor(train$Target) 

# Fitting SVM to the training data
# Using radial basis kernel (can use others like linear, polynomial)
#for reproducibility
set.seed(1217)
svm_model <- svm(Target ~ ., data = train, method = "C-classification", kernel = "radial")

# Summary of the model
summary(svm_model)
```

Testing the SVM

```{r}
# Making predictions on the test data
svm_predictions <- predict(svm_model, newdata = test)

#Convert predictions to factor
svm_predictions <- factor(svm_predictions, levels = levels(train$Target))

#Ensure test target is a factor w/ same levels
test$Target <- factor(test$Target, levels = levels(train$Target))

svm_predictions <- predict(svm_model, newdata = test)

# Confusion Matrix to evaluate performance
confusionMatrix(svm_predictions, test$Target)
```

## Test using cleaned test and train sets for SVM

```{r}
# Convert target variable to a factor
train_clean$Target <- as.factor(train_clean$Target) 

# Fitting SVM to the training data
# Using radial basis kernel (can use others like linear, polynomial)
#for reproducibility
set.seed(1217)
svm_model <- svm(Target ~ ., data = train_clean, method = "C-classification", kernel = "radial")

# Summary of the model
summary(svm_model)
```

```{r}
# Making predictions on the test data
svm_predictions <- predict(svm_model, newdata = test)

#Convert predictions to factor
svm_predictions <- factor(svm_predictions, levels = levels(train$Target))

#Ensure test target is a factor w/ same levels
test$Target <- factor(test$Target, levels = levels(train$Target))

svm_predictions <- predict(svm_model, newdata = test)

# Confusion Matrix to evaluate performance
confusionMatrix(svm_predictions, test$Target)
```
